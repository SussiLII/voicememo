What kind of predictors would we think about? What is the class of predictors? One important class of predictors that we'll talk about today are linear predictors. For example, it could be that you state your input x and apply a linear function to it, indicated by the red line there. Okay? And so, you know, these are the green points. These are the, this is the training data. It says for value 1, you should have gotten value 1. If the value was 2, you should have gotten value 3. If the value was 4, you should have gotten value 3. Right? This is what the training data is. The red line is our prediction. So if we get input 3, we're going to look at which point in the red line corresponds to x equal to 3, and that's the prediction. Okay? All right, so we're going to make a design decision. What other predictors will we allow? That's the hypothesis class. Then we can say how good of a predictor is this? We're going to see how well do these predictions fit the data that's available to us. And then, we're going to then ask the question, okay, fine, now that we've decided this, how do we actually find the best predictor? How do we find, within this class of predictors, what's the algorithm, sorry, what's the predictor that actually does well as far as this particular loss function? Okay, so these are the design decisions that we have to make as we go along. Okay, so let's start with a very simple class of predictors. Just linear predictors. So what are linear predictors? They're just a straight line here. So for now, let's say that my inputs are just real numbers. So here are my inputs on the x-axis. Here are the outputs on the y-axis. And I can think about the red predictor, which is this red line. It's given by this function f of x is 1 plus 0.57. That's the red line. You can also think of another one, the purple line, which has a y-intercept of 2 and a slope of 0.2. And this is 2 plus 0.2. In general, any predictor in this class can be represented by w1 plus w2 times x. So I'm going to introduce some notation. It's going to be useful for us. It will make my job a little bit easier and our job a little bit easier to think about. We can represent a predictor by two parameters, w1 and w2, just the coefficient here, the constant coefficient and the coefficient of x. That's w1 and w2. I'm going to introduce this notation which sounds a little heavy right now, but it's going to be useful later. Our input is just x. convert x into a feature vector, and for now, just think about my feature vector as one . So x is just a real number, but I'm going to feed x into my feature extractor, which is admittedly overkill at this point. It can produce just a vector, and the vector has just two components . The nice thing about this is that this linear function that I described over here on dot is just an inner product between this weight vector and this feature vector. So, I just introduced all of the notation, but all it does is encode the fact that the inner product of w and t of x is exactly this. Well, what is the prediction for 3? Well, for that, you have to look at the weight vector, 1 and 0.57, and take this as the one that you're using. Take the inner product with the future vector for 3, which is 1.3, and then have it to be 0.71. That's our prediction. Now, the hypothesis class is the set of all possible functions f of w. where f of w is of this form. So all possible w. And what are w? W are two-dimensional objects. So two parameters, w1 and w2. So now we talk about what the class of function is. We talk about how good a predictor we have. Well, that basically is just some measurement of how well the predictor matches up with the data that's given to us. So we have the training data, which are these green points. There are these xy pairs. And those are the green dots here. Our function is w dot p of x. So what we want to know is how well does this match? How well do our predictions match the data? So one way to do that is to say, let's between the prediction and the data, and let's do something different. So an actual thing would be to take maybe the absolute value. We're actually going to take the square. The square turns out to be very convenient for the nice mathematical properties. There's some situations where you might actually want to take absolute value, but it turns out that the square actually has some nice properties that are useful. So we're going to take the square. So basically what we're doing is we're looking at these dotted lines, looking at the length of these dotted lines, and we're squaring them. So we'll call this a loss function. So it takes some getting used to the terminology. The terminology loss function is just measuring how well our predictor matches the data. So for every data, for every example, we can compute the loss. What's the loss? The loss for the example 1, 1 is exactly this function. You take 1, 1, take the inner product of 1, 1 with w1, w2, which is 1, 1, 5, 7. Subtract the actual value, the desired value of 1, and take the square, and divide by . OK, you can check. And you can do this for each of the three examples. We have a loss for each of the three examples. What's our training loss? Our training loss is going to be the average of the losses over all of the examples in our data set. So this is just a measure of how well the data is. There are different ways in which you might measure this. I'm going to measure it this way. So if you run this, the trading loss on this particular data set for this particular predictor .38. to visualize the loss function. Because the loss function is just a function of the weight vector, W1W2. It's a two-dimensional function. It's a function that takes a two-dimensional vector and produces a value. That value is given by this. And our goal now, my goal, is to find that weight vector that minimizes this particular loss. And, you know, this loss function, you can kind of play around with it on the slide. It's sort of a quadratic circle. But our goal now is To get the training data, we've sort of encoded it in this loss function. Now, our goal is just to find this two-dimensional vector, W1, W2, that minimizes . We've converted our learning problem now into an optimization problem. So how do we find? That brings me to the third component, which is, how do we optimize? So in order to find the best choice of parameters, we're going to do gradient descent. So how do you do this? First of all, if you have this particular surface, you're at some point on the surface, you have some value w1, w2. Let's look at the direction in which the gradient loss increases the most. That's called the gradient. The gradient of the function is just the direction you need to move it, which means the function increases the most. We actually want to move in the opposite direction. So what we're going to do is we're going to perform this natural algorithm for gradient descent. We're going to start with the all zeroes vector. And then we're going to run for a number of steps. 1 through t. t is going to be a parameter that we have to set somehow. And what we're going to do is we're going to take our current weight vector, we're going to update it by a tiny amount for the step size times the gradient of the training line. So again, I said the gradient of the training loss is the direction in which the training loss increases the most. Negative of the gradient is the direction in which the training loss decreases the most. So we're going to move in that direction, but we want to sort of take a small step. So we're going to multiply that gradient by theta, which is going to be the step size. And so we start out here, take a little step, take another step, take another step, and sort of bounce around the surface and hopefully you end up okay now you know there are a few things that we need to set we need to set what the number of iterations is what the number of epochs is what is the step size uh you know do you change the step size dynamically do you keep it fixed these are all decisions all right okay any questions okay okay so how do we conclude this gradient so It turns out that for this particular loss function that we chose, the gradient has a very nice form. So remember, this was the gradient loss. It was the average over all the examples of the difference in our prediction and the actual target value squared summed up over all the examples. So I'll let you brush up on this. If you actually apply the chain rule to this, the gradient of the gradient loss is just the sum of any examples. The gradient is also the sum of any examples. And this term here, the gradient of this term is this very nice thing, which is it's something times p of x. p of x, remember, is a feature vector, for example, x. And what do you multiply p of x by? Well, you want to have A of x by exactly, well, 2, 2 times y, because this is a square, and this expression. This expression just says the prediction minus the time. So that doesn't have to be the gradient. So this is very easy to do the gradient just by just doing the expression. If you had some other expressions, it would have been enough, and it would have been enough to do the gradient. So that's the expression. So let me just work out some examples for you. So what happens when you perform training tests? So here's our training data. Three examples. This was the training loss. And this was the gradient update. And I've just set my step size to be 0.1. You might have other choices. So let's see what happens. Initially, we start with 0.0. Okay. All right. So you start with 0, 0, and say, let's compute the gradient of the gradient. So what that amounts to is an average over all the examples . So, you know, for the example 1, 1, 1, it's some expression times 1, 1. So this one is going to be what? T of x, T of 1 was 1 comma 1. T of 2 was 1 comma 2. T of 4 was 1 comma 3. So this is T of 1, T of 2, T of 3. And if you remember from the expression, I'm going to take twice w dot T of x minus y minus y dot T of x. So that's exactly the expression. I'm going to do it here, here, and here. You can do this calculation. This is the value that So this is the value that we get for the average, for the trading loss. Now, what is the update? The update is minus 0.1 times the trading loss. So if you apply this update to this vector, you get this. So we started with 0, 0. Our new vector is going to be 0.47, 0.25. Now it's 0.5. Same algorithm again. I'm not going to go through it again. But this is the calculation. This is the outcome. This is the new vector. And you go on again and so forth. And so if you actually perform it 200 times, it turns out you get to this particular vector. So you have this on the website. This is what looks like. So let's start with this example, exactly the three examples that we had before. So I have a vector with all the examples. They have the value x and the desired value y. So that's 1, 1, 2, 3, 4, 5. They're exactly what I have. I'm going to define my feature vector p of x. It's just going to be a two-dimensional array which has 1 on x. So that's p of x. My initial weight vector is zero. That's this one. And I define my training loss. And it's exactly just the expression I had before, which is I take the average over the training examples of the difference in the prediction of the target squared, summed up over all of the examples in my data. So that's the training loss. What's the gradient of the varying loss? Well, it's the same question I had before. Again, it's the average now of two times the difference between the prediction and the target times the events. So that's . OK, good. So having defined this, now we can So notice that there's a decoupling between setting up the optimization problem and then running and then the optimization algorithm. So this was the optimization problem that we set up. Now let's define what the optimization algorithm is. So what's the optimization algorithm? This is the one I have to do this first. What does gradient descent do? We're going to start with the initial weight vector, the step size. So now I'm just going to fix to 0.1. I'm going to run 500 . So capital P, I'm going to call it 500. um i'm going to start with uh i'm going to compute the value because i want to report it and we compute the gradient and then i'm going to do the update okay and i'm just going to print out what epoch i'm at um what the current w is what the value of the gradient is and that's what we're going to present this and i'm actually going to call it So just to give you a sense of . So if we run gradient descent, On the example, you can see we start out equal to 0. We have this. You can look at the values. And the values are dropping, dropping, dropping. And then you get down. you get down to this value of 0.38. We can work 0.38 much earlier than that. And the vector w that we obtain is actually just 1 and 0.57. That's what gradient descent looks like. OK. We'll come back to this. All right, so to summarize, we have our training data. We have a learning algorithm which trains this predictor f. Our predictor was just this linear function. So we answered all of these questions. We said, what predictors are possible? We chose linear functions. How good is the predictor? We defined the loss function to do that, and we looked at the squared loss. And then how to compute the best predictor? Well, we just decided to use gradient descent. and this is what linear regression is. Any questions? Okay, good. So now let me talk about linear classification. So in linear classification, Instead of having a real value, our output is binding. You're just going to say yes or no. So for example, now my examples are going to be two-dimensional. I'm going to choose, let's say I have, this is my pairing data. So I have, for each example, I have x1, x2. It's a two-dimensional vector. And I have a desired value of 1. So... Here, I have the vector 0, 2, and the desired value is 1. Here, I have the vector minus 2, 0, the desired value is 1. 1 minus 1, the desired value is minus 2. I'm going to feed this to my learning algorithm. My learning algorithm is going to do something, design, and come up with a classifier. What does the classifier have to do? The classifier will take new examples, such as 2, 0, and output a label. In this case, let's say the label is minus 1. So one way to think about what this classifier is is it's helpful to think about the decision boundary. If we look at the two-dimensional space of interest with x1 and x2, what inputs does the classifier output plus 1 for versus what inputs does the classifier output minus 1 for? And what exactly is the boundary between them? That's the decision boundary. So in this case, I have a linear decision boundary. I have other decision boundaries. Second, how good is the classifier? What's the loss function? Similar to what we did before, what's the loss function? And then, how do we compute the best classifier? Once we answer those two questions, we're gonna run our . Okay? So we're gonna do exactly what we did before. Okay, so a very natural class of classifiers are simply linear classifiers, which have linear decision models. Okay, so let me define them. So what we're going to do is very similar to what we did before. We're going to take w, which is going to be a two-dimensional vector. We're going to take p of x. In this case, p of x is just x itself. Remember, the input now is two-dimensional. p of x is just x itself, which happens to be a two-dimensional vector. We're going to take the inner product, and the inner product is a real value. We're going to take sine of the inner product, which is now going to be plus 1 minus 1. Of course, it could be 0, or it could be 0. So let's not worry about it. So sine of this expression is going to be plus 1 if it's positive, minus 1 if it's negative, and 0 if it's negative. So this was our training data. Let's look at this particular function given by this particular weight factor. It happens to be, as you can see, the decision boundary happens to be this red one. So let's think about what's going on. This vector w minus .6 comma .6, if you think about what vector it is, it happens to be this . Okay, so how should we think of this function acting on the input 0 comma 2? So if you look at 0 comma 2, 0 comma 2 is this vector here. This vector is this vector here. You're taking the inner product and asking for what the sign is. Basically, what you're asking is, what's the angle between these two vectors? Is it less than 90 degrees or bigger than 90 degrees? If that angle is acute, then this sign is going to be positive. The output is 1. Let's do the same thing for this, the second input. The second input is minus 2, 0. That's this vector here. When you take the inner product and ask for what the sine is, again, you're asking, is the angle bigger than 90 or less than 90? So in this case, the angle is a little cute. So again, the output is 1. So the third example, the vector is 1 comma minus 1. That's this one. Now in this case, when you take the sine of this vector times this one, that's going to be negative because the sine is 180. All right. And what's the decision boundary? It's exactly the set of points for which the angle between this vector and those points is 90 degrees. So this vector is the normal . All right? Any questions? Yeah? . The decision boundary is exactly the boundary of the points for which the pacifier . Secondly, we can apply it for other things. When the output of the pacifier changes from one class to another class. That's the decision boundary. For now, OK, let's do the following. For now, all the discussion is about binary classification. So let's restrict the binary classification. OK. Other questions? Yes. How do we do something that's ? You make an arbitrary decision to say, I'm going to take a consistent decision to say, I'm going to predict plus 1 or predict minus 1. Other questions? OK. All right. So what we've done now is we've answered this question of which classifier. So I looked at one particular classifier given by this weight vector minus 0.6.6. But there could be other weight vectors I could pick. For example, I could pick the weight vector 0.51 that corresponds to this purple left figure. and it has a different decision value, this purple line. So you have the red line, you have the purple line. In general, you could use any weight vector w, and that gives rise to a different decision value. The set of all such classifiers is this one, and that's our hypothesis. This is the set of classifiers . All right. So how good is our classifier? We're going to ask this question. Now, this question maybe is not as straightforward as it was before. So we're going to ask the question, how good is our classifier? So again, we have our training data. We have our classifier, which now takes w times p of x and returns a sign. All right. One natural question, one natural way to define how good a pathway is, is to ask, does the output of the pathway match the desired pathway? So apply . OK, so we're going to just ask, Look at the output that a classifier gives us. Look at the target. Do we match the target or not? If we match the target, the loss is zero. If we don't match the target, the loss is zero. This is a simple loss function called the zero-one loss function. So what are we doing? If you want to compute this loss function, 0.02 with target value 1 for a weight vector 0.51. What we're going to do is we're going to say, apply the classifier with weight vector 0.51 to the example 0.02 and see what happens. So what does the classifier do? It takes the inner product between these two vectors and reports the sign. And what we're going to check is, does that sign equal to plus 1 or not? If it's not equal to 1, then I return 1. I go ahead and return 0. So in this case, I return 0. And that's because as far as the purple line is concerned, the first example is correctly classified. It's on the positive side. The second example, on the other hand, is on the negative side. It should have returned a value of 1, but my classifier returned a value of minus 1. So for that, the loss is 1. And the third example is indeed on the negative side. Our classifier returns minus 1. The target value is also minus 1. So our loss is 0. So we have a loss of 1 for one example. We average over three examples. So our training loss is 0.2. Alright, okay, so a few other things we need to keep track of. So I'm going to define, so far what we've done is we've defined what the prediction is. Prediction is the sign of the inner product being W by T of X. It's also an assumption of how sure we are, how confident we are. The higher the magnitude of W by T of X, the more confident we are in R, at least the more confident R predictor is in its assessment, in its prediction. So I'm going to find the score to be the actual value before we take the sign of W times T to the X. It's a measure of confidence. And I'm going to find the margin to be W times this score times the true label of Y. So if our score is very positive, this means that we got the answer right, and we were very confident. If the score is very negative, it means we were very confident, but we got the answer wrong. So it's a measure of how confident we are, and the science tells you whether you got it right or not. Now let's look at that zero one loss that we had before. Let's rewrite it in terms of the score and margin. So earlier we said, we're just going to check if If the margin is positive, you got it right. The last function is just checking to see if the margin is less than or equal. So if you look at this function as a function of the margin, it's a very simple function, which is 1 if the margin is negative, and then 0 if the margin is positive. All right, good. So now, once we've defined our loss function, we can run our optimization algorithm. It's going to be the same optimization algorithm we did before. We're going to run it in the same way, by computing the training loss, by averaging over all of our training examples, the loss function,