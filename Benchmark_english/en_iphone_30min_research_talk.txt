a leading researcher from either industry or academia to come speak about some state-of-the-art topic they're working on in Transformers. So we have an exciting lineup of speakers prepared for you guys this evening. a section on pre-training and data strategies, and then a section focused more on post-training, which has become a very popular topic these days. We'll also touch briefly on some applications of transformers and some remaining weaknesses or challenges that we should hopefully address to be able to further improve the state of AI and our machine learning model. Yeah, so we'll start with some instructor introductions. So we have a very good team of co-instructors. So my name is Steven. I'm a current third year CS PhD student. Previously did my undergrad at Waterloo in Canada. I've done some research in industry as well at Amazon and NVIDIA. And in general, my research sort of focuses, hovers around natural language processing, so machine learning for language and text. full ability and reasoning models of large language models. And more recently, cognitive science and psychology inspired work, especially bringing the gap, the data gap and the learning efficiencies between machine learning models and how the humans learn, how human children learn and how our brains are able to learn so efficiently. I've also done some work with multi-modal as well as computer vision, so things like diffusion models and image generation. And just for fun, I also run the piano club here with Karan, and we have an upcoming concert on April 11th. engineering Ph.D. students. I did my undergrad at Cal Poly San Luis Obispo, after which I was a research scientist here, not doing my Ph.D. I'm a little bit more on the medical imaging side, so my current work is at the intersection of computer vision and neuroscience, working with things like fMRI and ultrasound. I currently work at the Hi everyone, I'm Chelsea. I'm a first year master's student in symbolic systems. And my general research interests are in multi-agentic frameworks, self-improving AI agents, and overall just kind of improving like the interpretability and explainability of models. So previously I studied applied math and neuroscience and I did a bunch of like interdisciplinary research in computer vision, robotics, cognitive science, things of that sort. And currently I'm working part-time at a VC firm and over the summer I'll be interning at a conversational AI startup as a machine learning engineer. So I'm very interested in exploring the startup scene here at Stanford, so feel free to reach out. Hi everyone, I'm Jenny. I'm a current student majoring in synthesis as well as a sociology co-term here at Stanford. My background is primarily in technology ethics and policy, so if you have any questions or want to talk about that, I'd love to have a conversation. In the past, I've worked doing product at DE Shoah and also researched in the tech ethics and policy space, and this summer I'll be working at Daydream, which is an AI fashion tech startup in New York. So Dib was unable to join us today, but he's working on his new agent startup called AGI Inc. Currently on leave from a CS PhD here. He's passionate about robotics, AI agents, and so forth. And later this quarter, he'll likely be giving a lecture actually on everything to do with AI agents. So if you're interested in that, definitely look forward to it. And previously, he's worked at Nvidia, Google, and so forth. And he's the one who sort of started this class in the first place. All right. So I'll go over some of the course logistics. So first announcement is we have a new website up. That's just cs45.stanford.edu. And so all of our updates, as well as the speaker lineup, will be posted there in the coming weeks. That will also be the link to share our Zoom with people who are not Stanford-related or are on the wait list or have not been able to gain admission to the class. So we encourage everyone to share this class with their network and ensure that anyone can access it from Zoom. Lectures are every Tuesday from 3 to 4.20 in this building. Attendance will be mandatory, and we'll start tracking that using a Google Forms link. Everyone can have three excused absences throughout the quarter. We encourage people to join via an in-person format if they can, although there will also be a Zoom link for those unable to join in person. There's no homework for the class, only attendance, so participation is encouraged and we're really excited to bring a cool host of speakers to the class. And then another announcement is we do have a Discord link. actually has over 5,000 members right now and we'd encourage you to join that to engage with people taking the class and also connect with people who might not be affiliated with Stanford, but engaging right now. And then throughout the quarter, we'll be streaming this class on Zoom as well as organizing at least one in-person meetup for the class to kind of connect with each other and debrief some of the topics that we've been discussing during the course. We'll be recording and broadcasting this live, so a couple of notes there are any questions that you ask during class will be deleted off of the live audio on the broadcast, so no worries about that. And if you have questions about your voice or your image, which will not be on the recording, you can come talk to us afterwards. So some other takeaways from this class include... Yeah, so some takeaways from the course include a better understanding of transformers and the underlying architecture of many of our large language models, guest speakers, which we'll be talking about applications in language, vision, biology, robotics, and more, exposure to new research, especially from new researchers all across the country, innovative methods that are driving the next generation of models, as well as key limitations, open problems, and Okay, next I'll give a really brief intro about transformers and how attention works. So the first step for language is word embeddings. So words aren't numbers, so we obviously can't just pass them into a model as is. So the first step is converting them into dense vectors in a high-dimensional space. This is done through various methods, but the goal is to capture semantic similarity. Essentially, that cat and dog are more similar than cat and car, even though the latter is more similar from a character standpoint. Doing so enables visualization, learning with transformer models, or arithmetic like I've shown, like king minus man plus queen would approximately be queen in some embedding space. And classical methods for this are like Word2Vec, FastText, and many more these days. But static embeddings, for instance, giving the word bank the same meaning, and just bank, as in river bank, have limitations. Therefore, the current standard is using contextual embeddings, which take into account the context and the Self-attention can be applied to this to learn what to focus on for a given token. So to do this, you learn three matrices, a query, key, and value, which together comprise the attention process. A quick analogy for this is imagine you're in a library looking for a book on a certain topic. This would be your query. Now, let's say each book has some summary associated with it. and get access to the book that you're looking for, the information inside the book would be your value. So in attention, we do a soft match over the values to get info from, say, multiple books, and this comprises the attention operation. And as you can see in this visualization, when you apply this to language, you can see that across different layers of the model, different words have connections to the rest of the words in the sentence. The next component is positional encodings or embeddings, which add order to the sequence. Without these, the model, since you have just linear multiplications here, would not know what the first or last word in the sentence is. Therefore, you add some notion of order through, say, sinusoids, or in the simplest form, you could think that the first word would be a zero and the second one a one. Beyond this is basically just scaling through multiple layers and multi-head attention. More heads to tend to different parts of the sentence and more parameters means that you can capture more diverse relationships for your sequences. And this gives you the final transformer. Transformers today have overtaken pretty much every field, from LLMs like GPT-4-0, O3 deep seek, to vision, with models that are getting increasingly better at segmentation, speech, biology, video. You'll see . With large language models, these are essentially just scaled-up versions of attention and the transformer architecture. You essentially just throw a large amount of data, general text data derived from the web, at these models, and they can learn very well to model through a next-token prediction objective language. And as you scale up, we've seen that immersion abilities pop up. So while on a smaller scale you might not be able to do a certain task, once you get to a certain scale you just have a peak in the ability to do that task. Some disadvantages though are that these models have very high computational costs and therefore also concerns like with climate and the carbon emissions they may produce. And like I was mentioning with larger models, they're very good at generalizing to make abilities or tasks, and they're essentially plug-and-play with fewer zero-shot learning. All right, so now I'll talk a bit more about pre-training. So as Karan explained how the transformer works, but typically with a language model, especially a large language model, you typically divide it into two stages. Pre-training stage, where you sort of train the neural network from scratch, from randomized or initialized weights, randomly initialized weights, to give it more general capabilities. And a big portion of this is the data itself. So the data is sort of the fundamental fuel your model to learn, because that's what the model is learning from. So your goal, typically, again, like I said with pre-training, is to train on a large amount of data to obtain some sort of general level of capabilities and overall knowledge or intelligence. And this is arguably, again, the most important aspect of training, especially pre-training. Especially because LLMs learn, again, based on statistical distributions predicting the next token given previous tokens. So to effectively learn this, you typically need a large amount of data. So because of its importance, how do we maximally leverage it? So again, smart data strategies for pre-training is definitely one of the most important topics these days. So I'll briefly touch upon two of the topic projects I recently worked on. on two different scales. The first is looking at what makes small, childlike data sets potentially effective for language learning, especially on the smaller scale. And the second is looking at smart data strategies for training large models on billions or trillions of tokens, which is on the much larger scale. So sort of why are humans able to learn so efficiently? This kind of looks at how human children learn and interact with the environment and learn language compared to a model like ChaiGBT, which is a bit analogous to how the human brain learns language and learns in general compared to something like a neural network. So some potential key differences are that humans learn continuously. We're continually learning. We don't just pre-train. We don't just sit in a chair, have someone read the whole internet to us, a lot of current models, which are more single-pass pre-training models. Further, we have more goal-based approaches to learning and interactions with the environment. That's a major reason we learn. Whereas, again, these models are typically just using next-book prediction or autoregression. Further, we learn through continuous multimodal or multisensory data. So it's not just text only. We're subconsciously exposed to probably hundreds of senses that guide the way we learn and approach our daily lives. Further, I believe our brains are fundamentally different in that we learn probably in more structured or hierarchical matters. For example, through compositionality rather than against simply next-book prediction. And the focus of this project in particular is more on the data differences. So again, humans are exposed to dialogue from people we talk to, storybooks, especially children growing up, compared to large amounts of data on the internet. So this is a work that was published This will really improve the efficiency of training and using live language models and this will open the door to potential new use cases. For example, models that can run on your phone that you can run locally and so forth for many different use cases. Smaller models and train on less data are also more interoperable and easier to sort of control or align, whether it's for safety purposes, to reduce bias, for safe reasons, and you have appropriate guardrails in place. This will also enhance the open source availability, allowing research and the usage of these models for more people around the world, rather than simply companies with large amounts of compute. And in general, this might even allow us to more greatly understand the other direction, which is how humans are able to learn so effectively and efficiently. Yep, so this work is titled Child-Directed Speech Effective Training Data for Language Models, which I presented at EMNLP in Miami last November. So again, the sort of hypothesis here is that children, you know, we sort of probably learn fundamentally different from LLMs. This is why we're able to learn on several magnitudes less language data in particular than many of these large language models these days, which require trillions of tokens. Now, there's several hypotheses. as humans is different, fundamentally from LLM, sorry. Rather than just training on internet data, we actually interact with people. We talk to people, we hear stories that our parents, our teachers tell us, and so forth. So our learning algorithm is just different from large language models. And another is maybe it's the way or the structure in which we receive this data. So any data we receive is somewhat curricularized. We start off with simple data, simple language as a child. And then learn more complex grammars, we hear more complex speech from our parents, co-workers, and so forth. Anything we do, whether it's learning math, we start simple and then move on to more difficult problems. Whereas language models, you typically don't care too much about ordering or curriculum. So there's multiple different hypotheses here. So in order to test some of these, what we did is we trained some small GPT-2 and RoBERTa models on five different data sets. One is Childis, which is a natural Conversation data with children, so this is transcribed. And then we collected a synthetic version called Tiny Dialogues, which I'll discuss more later. BabyLM, which is a diverse mixture of different types of data. This includes Reddit data, Wikipedia data, and so forth. So this is closer to your typical large language model pre-training data. And then we also did a bit of testing with Wikipedia as well as open subtitles, so movie and TV transcriptions. So we collected tiny dialogue, and this was inspired by the fact that a lot of, again, I said our learning as children is through conversations with other people. And conversations naturally lead to learning. We talk to someone, they give us feedback, we reflect on how the conversation went. So it's both pure and self-reflection. Furthermore, conversations lead to not only learning of knowledge, but other things like ethics and morals. For example, parents or teachers telling us as children, you know, on what's right or wrong to do. And there's many different types of conversations you can have with many different types of people, leading to a lot of diversity in learning. So what we did is we collected a fully grammatical and curricularized conversation data set with a limited childlike restrictive vocabulary using GPT-4. And we collected different examples that differ by child age, the different participants in the conversation, and so forth. And here's just some examples of some data points in our collected data set. So you see as the age goes up, you know, the utterances or conversations become more complex, they become longer. The participants also differ by age appropriately. So we also ran an experiment, a curriculum experiment, where we ordered the examples that the model sees. either by ascending age order. So the model will first see two-year-old conversations, and then five-year-old conversations, and then 10-year-old, and so forth, versus descending order. Maybe it's possible a language model might actually learn somehow better from more complex examples first. And then, of course, the typical baseline of randomly shuffling all your data examples. So we have some basic evaluation metrics targeted at the fundamental capabilities. One is basic grammatical and syntactic knowledge, and another is a free word association metric called word similarity for assessing more semantic knowledge. So you see here from the different data sets that actually it seems like training on child-like data is worse than a heterogeneous mixture of just internet data like BabyLM. So both metrics degrade quite substantially, especially on child-less, the more natural conversation data set between children and their caregivers. And you'll see in terms of curriculum, we don't see many substantial differences, no matter what order you sort of provide the examples into the model, which is again, sort of go from simple to more difficult. So looking more closely at sort of convergence behavior or loss curves, you'll see here that the training loss, the training loss sort of has these sorts of a cyclical pattern, depending on the sort of buckets you use for curriculum. But the validation loss, which is what you really care about, so the generalization and learning, it has the exact same trend no matter what order you feed interesting sort of finding. So overall, we see that diverse data sources like BabyLM seem to provide a better learning signal for language models than purely child-directed speech. We do see, however, that our tiny dialogues data set noticeably outperforms the natural conversation data set, likely because that data set is very noisy, whereas ours is, again, synthetically collected by GP4. And again, global developmental ordering using curriculum learning seems to have negligible impact on performance. So overall, we can kind of are exposed to are responsible for their efficient language learning. For example, learning from other types of information, like multimodal information, or it's the fact that our learning algorithm in our brain is just fundamentally different and more data efficient than language modeling techniques. So if you wish to learn more, we have our data sets released on Hugging Face as well as GitHub and the papers up on Archive.us. So now let's go bigger scale. So we were investigating small models trained on small amounts of data similar to a human child. Now what about current large models, billions of parameters trained on trillions of tokens? So I recently, during my last summer internship, I worked on a project with NVIDIA titled Maximize Your Data's Potential, enhancing LLM accuracy with two-phase pre-training. So this is the sort of optimized data selection scale of pre-training. So a lot of works like Longa highlight the effectiveness of different sorts of data mixtures. But they don't really shed light into the exact mixtures and how these decisions were made. Whereas we know data blending and ordering is crucial to effective LLM pre-training. So can we shed more light on this, which is what our work does? So firstly, we sort of formalize and systematically evaluate this concept of two-phase pre-training. And we show that empirically it improves over continuous training. or a different schedule. We also do a fine-grained analysis of data blending for these two pre-training phases. And we sort of have this notion of prototyping blends on smaller token counts before scaling up. So this two-phase pre-training approach is sort of inspired by how pre-training and post-training works. The first phase is on more general data. So this is to learn more broad. So it's on more diverse data. And the second is to show specific data such as math and so forth. However, it's important to sort of balance between quality and diversity in both phases, as if you up-weight any data set too much, it can lead to over-fitting. So firstly, the phase two phase training actually helped. So we found that all our phase two blends, up from the baseline of simply just continuing training on a single phase. And this is noticeably better than just a randomized mixture of both phases, as well as the natural data distribution compared to our sort of up-sampled data distribution for phase two. And we also showed that this is able to scale, both on model scale and data scale. So if you blow up the token counts as well as the model size, we show that performance compared to a single phase. So this kind of highlights also the effectiveness of prototyping on smaller data blends before scaling up. And furthermore, we investigated the duration of phase two. So should we train on diverse data for a little bit and immediately switch to highly specialized data like math, or should we wait longer? And what we found is performance improves up to a point around 40%. returns likely from overfitting. Because specialized data, it's more specialized. There's typically a lower number of it, and it's less diverse compared to things like web call data. So too much of it can lead to detrimental or diminishing returns. So overall, we see a well-structured two-phase pre-training approach with careful data selection and management is essential for optimizing LLL performance while maintaining scalability and robustness across different downstream . And in case you're interested, this paper is also pre-printed as an archive. And what I wanted to get at is the fact that data effectiveness, especially for pre-training, it's not just the amount of data, but it's about the quality of the data, the ordering and structure of data, and how exactly you use it. So for our first project, we saw this negative impact of global order in small-scale training. But we saw that phase-based training for larger scales is highly effective. And in general, small data decisions are essential for models to generalize across tasks. So sort of takeaway is, our research underscores that effective language modeling isn't just about amassing data, but about smarter data organization that harnesses its structure, quality, and characteristics. And by continuing to sort of refine data-centric approaches, the future of LLM training promises smarter, more efficient, and highly adaptable math. So now we'll be moving to sort of the second stage after pre-training, which is post-training, which Chelsea will talk about. All right, so we have a pre-trained model, now what? Like how do we adapt to specific tasks and different domains? So some major strategies include fine tuning, for instance, like reinforcement learning with human feedback, or some prompt-based methods, or some sort of like RAG architecture and retrieval-based methods. So one major approach is called chain of thought reasoning. I'm sure you all have heard of it by now. So it's essentially a prompting technique to think step by step. So it shows the intermediate steps provide guidance. And this is sort of similar to the way how humans think. We can imagine that we typically break down a problem into subsequent steps to help us better understand the problem itself. And another benefit of chain of thought is that it allows some sort of interpretable window into the behavior of the model. And this can kind of suggest that there is more knowledge embedded in the model's ways than just prompting a response. So this here is an example of chain of thought. On the left, we have it solve a problem in a one-shot manner, which turns out to get to the wrong answer. And on the right over there, it produces a sequence of reasoning chains, and ultimately it arrives at the correct answer. So naturally, this brings up an extension of chain of thought, which is called a tree of thought. And this is another pumping technique, but instead of producing a singular reasoning path as a chain of thought does, it considers multiple reasoning trajectories and then uses some sort of self-evaluation process to kind of decide on the final output, such as majority voting. So in the picture, you can see that tree of thought kind of generates different reasoning paths and selects the best one at the end. So another way is through program of thought, and this basically generates code as the intermediate reasoning steps. And overall what this does is that it offloads some sort of problem solving technique to some code interpreter. So it formalizes language into programs to arrive at more precise answers. So we have seen that this sort of problem decomposition seems helpful for different tasks. So one way is through Socratic questioning, which is basically using a self-questioning module to propose sub-problems related to the original and solve those in a recursive sort of manner. So for instance, if the question is what fills the balloons, this leads to the next sub-question, which is what can make a balloon float. And then by decomposing the original problem into subsequent problems, it can better solve at the end.